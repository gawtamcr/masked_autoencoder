{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmklZXemhspB"
      },
      "source": [
        "# SimCLR\n",
        "\n",
        "In this practial exercise, you are going to implement the famous SimCLR [1] algorithm for self-supervised learning. While not that old, it is often seen as the 'classic' approach to contrastive learning and it is still used as a baseline against which newer approaches are compared. Most of the more recent algorithms are just slight alterations of this approach, so knowing how it works will pay off in the future. Due to the growing amounts of data, self-supervised learning is likely to become more and more relevant in the future.\n",
        "\n",
        "Or as Yan LeCun and Ishan Misra put it: SSL may be helpful to unlock the [dark matter of intelligence](https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/).\n",
        "\n",
        "## Getting labels is expensive and can be ethically difficult\n",
        "\n",
        "Regular supervised learning requires each data point to be labeled with ground truth information. Additionally, it requires large amounts of data. This unfortunately means that humans have to spend a lot of time hand-labeling data before we can train our networks. If determining the ground truth requires expert knowledge (e.g. diagnosing a disease based on a brain scan), getting large amounts of data is very expensive and time-consuming. In other cases, it can be very repetitive, mind-numbing work: \"Click all images containing traffic lights.\" Since nobody wants to do this kind of work, it is often outsourced to crowd-workers who earn very little and have generally bad working conditions:\n",
        "https://thegoodai.co/2021/02/03/who-is-labeling-your-data/\n",
        "\n",
        "## SSL is a form of pretraining\n",
        "\n",
        "Self-supervised learning now takes a bit of a different approach than supervised learning. It simply outsources the labeling to the computer! The computer *supervises itself*.\n",
        "\n",
        "We first pretrain our model on a so-called *pretext task* using a large **unlabeled** dataset, then fine-tune it on a smaller, labeled dataset. The pretext tasks often consist of supervised learning, but use labels that the computer can automatically generate for any image. Afterwards, we finetune our model using regular supervised learning on a smaller dataset.\n",
        "\n",
        "One example for a pretext task would be to rotate the input image and let the network predict the degree of rotation. There's no need for human labeling, since the program has the information about how it rotated the image, so it can use that as a label right away. To solve the task, the network needs to learn e.g. that the sky is usually at the top of the image and that cars have their wheels on the bottom of the image. So it is forced to learn to recognise what the image contains, which is probably useful for the downstream task!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWk3Os9GrTIC"
      },
      "source": [
        "**Task 1** Come up with a simple pretext task yourself for any kind of dataset you like and describe it in a sentence or two. For example: \"For pretraining a dataset of dog images, I would rotate the images and let the network predicted how the image was rotated.\" But don't simply use rotation, come up with something yourself!\n",
        "\n",
        "Hint: Your program can change anything about the image and use information about that change to create a label."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14qeLONXsriG"
      },
      "source": [
        "**Answer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMhB8RRhhZQN",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "# Preparation\n",
        "\n",
        "No need for you to implement the boring stuff. Just run the cells, we'll need it further down."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "jax.config.update(\"jax_debug_nans\", True)"
      ],
      "metadata": {
        "id": "LBfRq3QthnQM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "P3HrlIokhToe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56d8c6d5-0b60-42c3-dd85-cb5f7e51f880"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jax-resnet\n",
            "  Downloading jax_resnet-0.0.4-py2.py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.12/dist-packages (0.2.6)\n",
            "Requirement already satisfied: umap-learn in /usr/local/lib/python3.12/dist-packages (0.5.9.post2)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.12/dist-packages (from jax-resnet) (0.7.2)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.12/dist-packages (from jax-resnet) (0.7.2)\n",
            "Requirement already satisfied: flax in /usr/local/lib/python3.12/dist-packages (from jax-resnet) (0.10.7)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from optax) (1.4.0)\n",
            "Requirement already satisfied: chex>=0.1.87 in /usr/local/lib/python3.12/dist-packages (from optax) (0.1.90)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.12/dist-packages (from optax) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from umap-learn) (1.16.3)\n",
            "Requirement already satisfied: scikit-learn>=1.6 in /usr/local/lib/python3.12/dist-packages (from umap-learn) (1.6.1)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.12/dist-packages (from umap-learn) (0.60.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.12/dist-packages (from umap-learn) (0.5.13)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from umap-learn) (4.67.1)\n",
            "Requirement already satisfied: typing_extensions>=4.2.0 in /usr/local/lib/python3.12/dist-packages (from chex>=0.1.87->optax) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from chex>=0.1.87->optax) (75.2.0)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chex>=0.1.87->optax) (0.12.1)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from jax->jax-resnet) (0.5.4)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax->jax-resnet) (3.4.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.2->umap-learn) (0.43.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.12/dist-packages (from pynndescent>=0.5->umap-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.6->umap-learn) (3.6.0)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.12/dist-packages (from flax->jax-resnet) (1.1.2)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.12/dist-packages (from flax->jax-resnet) (0.11.28)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.12/dist-packages (from flax->jax-resnet) (0.1.79)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.12/dist-packages (from flax->jax-resnet) (13.9.4)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.12/dist-packages (from flax->jax-resnet) (6.0.3)\n",
            "Requirement already satisfied: treescope>=0.1.7 in /usr/local/lib/python3.12/dist-packages (from flax->jax-resnet) (0.1.10)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1->flax->jax-resnet) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1->flax->jax-resnet) (2.19.2)\n",
            "Requirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax->jax-resnet) (1.13.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax->jax-resnet) (1.6.0)\n",
            "Requirement already satisfied: aiofiles in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax->jax-resnet) (24.1.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax->jax-resnet) (5.29.5)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax->jax-resnet) (4.14.0)\n",
            "Requirement already satisfied: simplejson>=3.16.0 in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax->jax-resnet) (3.20.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax->jax-resnet) (5.9.5)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax->jax-resnet) (0.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax->jax-resnet) (2025.3.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.12/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax->jax-resnet) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.12/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax->jax-resnet) (3.23.0)\n",
            "Downloading jax_resnet-0.0.4-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: jax-resnet\n",
            "Successfully installed jax-resnet-0.0.4\n"
          ]
        }
      ],
      "source": [
        "%pip install jax-resnet optax umap-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hLuiJEqAv-Ei"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "from jax import random\n",
        "import numpy as np\n",
        "from torch.utils import data\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, normalize\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import umap\n",
        "import jax\n",
        "import optax\n",
        "from flax.training import train_state\n",
        "from typing import Any\n",
        "import tqdm\n",
        "import jax.numpy as jnp\n",
        "import jax_resnet\n",
        "import flax.linen as nn\n",
        "from flax.core.frozen_dict import freeze, unfreeze"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ll_OnOlThl-U"
      },
      "outputs": [],
      "source": [
        "# Augmentation\n",
        "\n",
        "# Slightly changed from source:\n",
        "# https://github.com/Spijkervet/SimCLR/blob/master/simclr/modules/transformations/simclr.py\n",
        "\n",
        "class TransformsSimCLR:\n",
        "    \"\"\"\n",
        "    A stochastic data augmentation module that transforms any given data example randomly\n",
        "    resulting in two correlated views of the same example,\n",
        "    denoted x i and x j, which we consider as a positive pair.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, is_pretrain=True, is_val=False):\n",
        "        self.is_pretrain=is_pretrain\n",
        "        self.is_val=is_val\n",
        "        s = 1\n",
        "        color_jitter = torchvision.transforms.ColorJitter(\n",
        "            0.8 * s, 0.8 * s, 0.8 * s, 0.2 * s\n",
        "        )\n",
        "        self.train_transform = torchvision.transforms.Compose(\n",
        "            [\n",
        "                torchvision.transforms.RandomHorizontalFlip(),  # with 0.5 probability\n",
        "                torchvision.transforms.RandomApply([color_jitter], p=0.8),\n",
        "                torchvision.transforms.RandomGrayscale(p=0.2),\n",
        "                torchvision.transforms.Lambda(np.array),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.test_transform = torchvision.transforms.Compose(\n",
        "            [\n",
        "                torchvision.transforms.Lambda(np.array),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def __call__(self, x):\n",
        "        if self.is_pretrain:\n",
        "          return self.train_transform(x), self.train_transform(x)\n",
        "        else:\n",
        "          if self.is_val:\n",
        "            return self.test_transform(x)\n",
        "          else:\n",
        "            return self.train_transform(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QEACTPr46DUv"
      },
      "outputs": [],
      "source": [
        "def compute_validation_performance(state, val_data_loader):\n",
        "  \"\"\"\n",
        "  Computes the given model's mean loss and accuracy on the validation set.\n",
        "  \"\"\"\n",
        "  val_losses = []\n",
        "  val_accs = []\n",
        "\n",
        "  val_dl_tqdm = tqdm.tqdm(val_data_loader)\n",
        "\n",
        "  for X, Y in val_dl_tqdm:\n",
        "\n",
        "    loss, accuracy = eval_step(state, X, Y)\n",
        "    val_losses.append(loss)\n",
        "    val_accs.append(accuracy)\n",
        "\n",
        "    val_dl_tqdm.set_postfix({'val_loss': loss.item(), 'val_acc': accuracy.item()})\n",
        "\n",
        "  return jnp.array(val_losses).mean().item(), jnp.array(val_accs).mean().item()\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def eval_step(state, X, Y):\n",
        "\n",
        "    logits = state.apply_fn({'params': state.params,\n",
        "                             'batch_stats': state.batch_stats}, X,\n",
        "                              mutable=False)\n",
        "    labels = jax.nn.one_hot(Y, num_classes=10)\n",
        "    loss = optax.softmax_cross_entropy(logits, labels).mean()\n",
        "    accuracy = jnp.mean(jnp.argmax(logits, -1) == Y)\n",
        "\n",
        "    return loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "D3GQ21tkwS6t"
      },
      "outputs": [],
      "source": [
        "# Data loader\n",
        "\n",
        "# Source: https://jax.readthedocs.io/en/latest/notebooks/Neural_Network_and_Data_Loading.html\n",
        "\n",
        "def numpy_collate(batch):\n",
        "  if isinstance(batch[0], np.ndarray):\n",
        "    return np.stack(batch)\n",
        "  elif isinstance(batch[0], (tuple,list)):\n",
        "    transposed = zip(*batch)\n",
        "    return [numpy_collate(samples) for samples in transposed]\n",
        "  else:\n",
        "    return np.array(batch)\n",
        "\n",
        "class NumpyLoader(data.DataLoader):\n",
        "  def __init__(self, dataset, batch_size=1,\n",
        "                shuffle=False, sampler=None,\n",
        "                batch_sampler=None, num_workers=0,\n",
        "                pin_memory=False, drop_last=False,\n",
        "                timeout=0, worker_init_fn=None):\n",
        "    super(self.__class__, self).__init__(dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        sampler=sampler,\n",
        "        batch_sampler=batch_sampler,\n",
        "        num_workers=num_workers,\n",
        "        collate_fn=numpy_collate,\n",
        "        pin_memory=pin_memory,\n",
        "        drop_last=drop_last,\n",
        "        timeout=timeout,\n",
        "        worker_init_fn=worker_init_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "lx05jkbspj-4"
      },
      "outputs": [],
      "source": [
        "# Source: https://colab.research.google.com/drive/1Y2IiAG69nKQSoIKAOAWC8uypdP3TGFqF?usp=sharing#scrollTo=TqDvTL_tIQCH\n",
        "def zero_grads():\n",
        "    def init_fn(_):\n",
        "        return ()\n",
        "    def update_fn(updates, state, params=None):\n",
        "        return jax.tree.map(jnp.zeros_like, updates), ()\n",
        "    return optax.GradientTransformation(init_fn, update_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "yHz8UtWz9gK6"
      },
      "outputs": [],
      "source": [
        "# Training functions\n",
        "\n",
        "# Adapted from source: https://github.com/google/flax/blob/main/examples/mnist/train.py\n",
        "# Parts that use batch norm adapted from https://github.com/google/flax/blob/main/examples/imagenet/train.py\n",
        "\n",
        "@jax.jit\n",
        "def apply_model(state, X1, X2):\n",
        "  \"\"\"Computes gradients, loss and accuracy for a single batch.\"\"\"\n",
        "  tau = 0.07\n",
        "  def loss_fn(params):\n",
        "    Z1, new_model_state = state.apply_fn({'params': params,\n",
        "                             'batch_stats': state.batch_stats}, X1,\n",
        "                              mutable=['batch_stats'])\n",
        "    Z2, new_model_state = state.apply_fn({'params': params, # We probably want to combine the two new model states, but how?\n",
        "                             'batch_stats': state.batch_stats}, X2,\n",
        "                              mutable=['batch_stats'])\n",
        "    loss = NTXent(Z1, Z2, tau)\n",
        "    return loss, (new_model_state, Z1, Z2)\n",
        "\n",
        "  grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
        "  (loss, (new_model_state, Z1, Z2)), grads = grad_fn(state.params)\n",
        "\n",
        "  new_state = state.apply_gradients(\n",
        "      grads=grads, batch_stats=new_model_state['batch_stats'])\n",
        "\n",
        "  return (new_state, Z1, Z2), loss\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def apply_model_pos_only(state, X1, X2):\n",
        "  \"\"\"\n",
        "  Computes gradients, loss and accuracy for a single batch,\n",
        "  uing only positive pairs.\n",
        "  \"\"\"\n",
        "  tau = 0.5\n",
        "  def loss_fn(params):\n",
        "    Z1, new_model_state = state.apply_fn({'params': params,\n",
        "                             'batch_stats': state.batch_stats}, X1,\n",
        "                              mutable=['batch_stats'])\n",
        "    Z2, new_model_state = state.apply_fn({'params': params, # TODO Amir: We probably want to combine the two new model states, but how?\n",
        "                             'batch_stats': state.batch_stats}, X2,\n",
        "                              mutable=['batch_stats'])\n",
        "    loss = PositivesOnlyLoss(Z1, Z2)\n",
        "    return loss, (new_model_state, Z1, Z2)\n",
        "\n",
        "  grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
        "  (loss, (new_model_state, Z1, Z2)), grads = grad_fn(state.params)\n",
        "\n",
        "  new_state = state.apply_gradients(\n",
        "      grads=grads, batch_stats=new_model_state['batch_stats'])\n",
        "\n",
        "  return (new_state, Z1, Z2), loss\n",
        "\n",
        "@jax.jit\n",
        "def apply_model_supervised(state, X, Y):\n",
        "  \"\"\"\n",
        "  Computes gradients, loss and accuracy for a single batch via supervised\n",
        "  training, not self-supervised training.\n",
        "  \"\"\"\n",
        "  tau = 0.5\n",
        "  def loss_fn(params):\n",
        "    logits, new_model_state = state.apply_fn({'params': params,\n",
        "                             'batch_stats': state.batch_stats}, X,\n",
        "                              mutable=['batch_stats'])\n",
        "    labels = jax.nn.one_hot(Y, num_classes=10)\n",
        "    loss = optax.softmax_cross_entropy(logits, labels).mean()\n",
        "    accuracy = jnp.mean(jnp.argmax(logits, -1) == Y)\n",
        "    return loss, (new_model_state, accuracy)\n",
        "\n",
        "  grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
        "  (loss, (new_model_state, accuracy)), grads = grad_fn(state.params)\n",
        "\n",
        "  new_state = state.apply_gradients(\n",
        "      grads=grads, batch_stats=new_model_state['batch_stats'])\n",
        "\n",
        "  return (new_state, accuracy), loss\n",
        "\n",
        "\n",
        "def pretrain_epoch(state, train_dl, positives_only=False):\n",
        "  \"\"\"\n",
        "  Train for a single epoch.\n",
        "  positives_only: Don't use negative pairs in contrastive training\n",
        "  \"\"\"\n",
        "\n",
        "  epoch_loss = []\n",
        "  epoch_accuracy = []\n",
        "\n",
        "  train_dl_tqdm = tqdm.tqdm(train_dl)\n",
        "\n",
        "  apply_fn = apply_model_pos_only if positives_only else apply_model\n",
        "\n",
        "  for step, ((X1,X2), Y) in enumerate(train_dl_tqdm):\n",
        "\n",
        "    (new_state, Z1, Z2), loss = apply_fn(state, X1, X2)\n",
        "    state = new_state\n",
        "    epoch_loss.append(loss)\n",
        "    train_dl_tqdm.set_postfix({'train_loss': loss.item()})\n",
        "\n",
        "  train_loss = np.mean(epoch_loss)\n",
        "  return state, train_loss\n",
        "\n",
        "def supervised_epoch(state, train_dl):\n",
        "  \"\"\"\n",
        "  Train for a single epoch with supervised loss.\n",
        "  \"\"\"\n",
        "\n",
        "  epoch_loss = []\n",
        "  epoch_accuracy = []\n",
        "\n",
        "  train_dl_tqdm = tqdm.tqdm(train_dl)\n",
        "\n",
        "  for step, (X, Y) in enumerate(train_dl_tqdm):\n",
        "\n",
        "    (new_state, accuracy), loss = apply_model_supervised(state, X, Y)\n",
        "    state = new_state\n",
        "    epoch_loss.append(loss)\n",
        "    epoch_accuracy.append(accuracy)\n",
        "    train_dl_tqdm.set_postfix({'train_loss': loss.item(), 'train_acc': accuracy})\n",
        "\n",
        "  return state, jnp.array(epoch_loss).mean(), jnp.array(epoch_accuracy).mean()\n",
        "\n",
        "\n",
        "class TrainState(train_state.TrainState):\n",
        "  \"\"\"\n",
        "  Keeps track of the optimizer and updated parameters.\n",
        "  \"\"\"\n",
        "  batch_stats: Any\n",
        "\n",
        "def create_train_state(model, params, config, freeze_encoder=False):\n",
        "  if freeze_encoder:\n",
        "    tx = optax.multi_transform({'zero': zero_grads(),\n",
        "                                'sgd': optax.sgd(config[\"learning_rate\"],\n",
        "                                                  config[\"momentum\"])},\n",
        "                                freeze({'backbone':'zero', 'head': 'sgd'}))\n",
        "  else:\n",
        "    tx = optax.sgd(config[\"learning_rate\"], config[\"momentum\"])\n",
        "  return TrainState.create(\n",
        "      apply_fn=model.apply, params=params[\"params\"], tx=tx,\n",
        "      batch_stats=params[\"batch_stats\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-jldvhKyQZe"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "We are going to use the CIFAR10 dataset. It consists of 10 classes, each represented by 6000 RGB-images of size 32x32.\n",
        "\n",
        "Have a look at a few images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "pvqiAtVByLcx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9936417-bd84-4661-8c52-496ba2c247f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:07<00:00, 22.4MB/s]\n"
          ]
        }
      ],
      "source": [
        "full_pretrain_dataset = torchvision.datasets.CIFAR10(\n",
        "        './cifar10',\n",
        "        download=True,\n",
        "        transform=TransformsSimCLR(is_pretrain=True, is_val=False),\n",
        "        train=True\n",
        "    )\n",
        "\n",
        "full_supervised_dataset = torchvision.datasets.CIFAR10(\n",
        "        './cifar10',\n",
        "        download=True,\n",
        "        transform=TransformsSimCLR(is_pretrain=False, is_val=False),\n",
        "        train=True\n",
        "    )\n",
        "\n",
        "val_dataset = torchvision.datasets.CIFAR10(\n",
        "        './cifar10',\n",
        "        download=True,\n",
        "        transform=TransformsSimCLR(is_pretrain=False, is_val=True),\n",
        "        train=False\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "eDDW-o7aojc7"
      },
      "outputs": [],
      "source": [
        "# Create a labeled dataset with only 1% of the labels\n",
        "import torch\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "sk = StratifiedKFold(n_splits=100, shuffle=True, random_state=0)\n",
        "splits = sk.split(np.zeros(len(full_supervised_dataset)), full_supervised_dataset.targets)\n",
        "_, train_idc = list(splits)[0]\n",
        "\n",
        "supervised_dataset_1p = torch.utils.data.Subset(full_supervised_dataset, train_idc)\n",
        "\n",
        "pretrain_dataset = full_pretrain_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "P3MnHp8l0H9t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "outputId": "f495d688-a89a-445f-c9f8-d9aaf0a6e35e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image x1 shape:  (32, 32, 3) \n",
            "Image x2 shape:  (32, 32, 3) \n",
            "class index y:  9 \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7ddc1e2cf590>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "(x1,x2), y = pretrain_dataset[1]\n",
        "print(\"Image x1 shape: \", x1.shape,\n",
        "      \"\\nImage x2 shape: \", x2.shape,\n",
        "      \"\\nclass index y: \", y, \"\\n\")\n",
        "\n",
        "merged_images = np.concatenate([x1,x2], axis=1)\n",
        "plt.imshow(merged_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDNcEEJR3t-l"
      },
      "source": [
        "You maybe have noticed that the transformation module we use directly gives us two images instead of one. They are always two differently augmented versions of the same source image. This will be needed for SimCLR, which teaches the network to recognise that these two images show the same object, despite the differences introduced by the augmentations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1TV2AUu5hG_"
      },
      "source": [
        "# Contrastive learning\n",
        "\n",
        "The basic idea of contrastive learning is to learn by comparison. In the following image, the model is supposed to learn that the first two pictures show the same object, but the third one does not. Kind of like in the children's game Memory, where you try to find two matching tiles. Just... without the memory part. All the tiles are turned face up.\n",
        "\n",
        "Contrastive learning is a really stupid version of Memory.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "5ol_liYy3s8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "5a423b02-5ef9-486a-bd63-f9ded0f27f42"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7ddc1d748ef0>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "(x00,x01), y0 = pretrain_dataset[1]\n",
        "(x10,x11), y1 = pretrain_dataset[4]\n",
        "merged_images = np.concatenate([x00,x01,x10], axis=1)\n",
        "plt.imshow(merged_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcFqHcaO6wNs"
      },
      "source": [
        "On a mathematical level, the model outputs a feature vector (an embedding) for each input image. The embeddings of similar images (like the two images on the left) are encouraged to move closer to each other, while embeddings of dissimilar images (middle and right image) should move away from each other.\n",
        "\n",
        "This approach is fundamentally different from supervised learning. While supervised learning prescribes the exact output that the model is supposed to produce for a given input, contrastive learning only cares about the *relationship between different outputs*.\n",
        "\n",
        "Image pairs that we, the user, define as similar are called **positive pairs**, all others are **negative pairs**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WZBPipS_QJK"
      },
      "source": [
        "## Similarity between images\n",
        "\n",
        "We want to be able to measure how close or far the embeddings for different images are, to see if our model can tell positive and negative pairs apart. In SimCLR, this is done using the cosine similarity. For two d-dimensional embeddings $z_1$ and $z_2$, it provides a measure for how similar they are.\n",
        "\n",
        "$cos(z_1,z_2) = \\frac{z_1}{|z_1|} \\cdot \\frac{z_2}{|z_2|}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwMAR6Rq7MBE"
      },
      "source": [
        "**Task 2** Implement the cosine similarity function below. It should also work if $Z_1$ and $Z_2$ are batches of shape [batch, features] and produce an output of shape [batch], containing the cosine similarities between the two feature vectors in the same row.\n",
        "\n",
        "Hint: Using for-loops here works, but is really inefficient. The efficient implementation operates on matrices, not on individual elements. It mostly looks like the equation above. If you get a problem with broadcasting between different shapes when you try to normalize $Z_1$ and $Z_2$, the function jnp.expand_dims might be useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "x9Y72aaMBxaD"
      },
      "outputs": [],
      "source": [
        "import jax.numpy as jnp\n",
        "\n",
        "def cosine_similarity(Z1, Z2):\n",
        "  \"\"\"\n",
        "  Z1, Z2: Batches of image embeddings of shape [Batch,EmbeddingDim]. Z1[i,:] and\n",
        "          Z2[i,:] represent two differently augmented versions of the same image\n",
        "\n",
        "  Returns: row-wise cosine similarity between row-vectors in Z1 and Z2,\n",
        "           shape [Batch]\n",
        "  \"\"\"\n",
        "  Z1 = jnp.asarray(Z1)\n",
        "  Z2 = jnp.asarray(Z2)\n",
        "  numerator = jnp.sum(Z1 * Z2, axis=1)\n",
        "  denom = jnp.linalg.norm(Z1, axis=1) * jnp.linalg.norm(Z2, axis=1)\n",
        "  denom = jnp.maximum(denom, 1e-12)\n",
        "  return numerator / denom\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE0_flzSCp7S"
      },
      "source": [
        "Verify your implementation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "p3f_d3ha7qZ7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f715ff78-c032-4dda-bf7f-987bddb4745b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# Use these sample inputs to verify that your implementation is correct.\n",
        "# You can interpret Z1 and Z2 as batches of image embeddings with eight features\n",
        "# per image.\n",
        "\n",
        "random_key_0, *subkeys = random.split(random.PRNGKey(0), 3)\n",
        "\n",
        "#Batch size = 4\n",
        "#Features = 8\n",
        "Z1 = jnp.array(\n",
        "    [[0.08780523,-1.4772962,0.23108286,0.3828757,0.42218092,-1.4358605,0.17658076,1.4141593],\n",
        "    [-0.43024102,-0.2960915,0.64877045,1.0952688,0.10887499,2.5801923,-0.46863857,-0.07292866],\n",
        "    [0.8462296,-0.9795519,-0.17020774,0.5177701,-1.2235159,0.56981355,1.1847981,-1.9526129],\n",
        "    [-2.2450106,0.8794637,0.2156571,0.22987445,-0.8855504,0.180139,0.75102454,0.79618496]])\n",
        "\n",
        "Z2 = jnp.array(\n",
        "    [[0.78220856,-0.95930517,0.04278377,-0.14640434,-0.45225152,-0.164141,1.0146061,0.19397569],\n",
        "    [-0.7706246,-0.35345954,-0.67818415,-1.1203834,-0.30522528,0.669694, 1.2020552,0.87423134],\n",
        "    [0.43238065,-0.18009177,-0.13709433,-0.33463678,-1.1886245,-0.35386798,-1.0499382,0.10795221],\n",
        "    [0.23042125,-1.5269405,0.771874,-0.1904757,-1.5630287,0.8980937,-1.9551364,-0.0497684]])\n",
        "Z1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "o90UHAJP8ENE"
      },
      "outputs": [],
      "source": [
        "your_output = cosine_similarity(Z1, Z2)\n",
        "desired_output = jnp.array([ 0.44447064, -0.02418898,  0.03658391, -0.19001874])\n",
        "\n",
        "np.testing.assert_array_almost_equal(your_output, desired_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAOXW2Xdq5k8"
      },
      "source": [
        "**Task 3** Wait a minute, why don't we just use the length of the unnormalized difference vector $|z_1-z_2|$ instead of the cosine similarity? Which problems could occur there that cosine similarity avoids?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yidDdY72Azuv"
      },
      "source": [
        "**Answer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkcxSyrE9Kkj"
      },
      "source": [
        "## Loss function\n",
        "\n",
        "This is the point in the exercise where you need to think a bit. While the loss function is not very complex, you do need to figure out how to translate the formulas into efficient matrix operations of the framework you're working with. There are different ways of doing this, pick the one that feels most natural to you, but please avoid for-loops. This is the core part of the exercise, and might take some time, depending on your previous experience.\n",
        "\n",
        "If you have tried for a bit and get stuck somewhere, feel free to use the hints below that are based on where the TA needed a bit longer for the implementation than he would have liked.\n",
        "\n",
        "**Task 4** Implement the loss function for a whole batch of image embeddings (equation 1 / most of algorithm 1 in the paper)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "0eovqgn4_kbi"
      },
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def NTXent(Z1, Z2, tau):\n",
        "  \"\"\"\n",
        "  Z1, Z2: Batches of image embeddings of shape [Batch,EmbeddingDim]. Z1[i,:] and\n",
        "          Z2[i,:] represent two differently augmented versions of the same image\n",
        "  tau   : Temperature parameter for softmax\n",
        "\n",
        "  Returns: NTXent loss between Z1 and Z2, a single number\n",
        "  \"\"\"\n",
        "  Z = jnp.concatenate([Z1, Z2], axis=0)\n",
        "  Z = Z / jnp.maximum(jnp.linalg.norm(Z, axis=1, keepdims=True), 1e-12)\n",
        "  sim_matrix = jnp.matmul(Z, Z.T)\n",
        "  logits = sim_matrix / tau\n",
        "  batch = Z1.shape[0]\n",
        "  pos_indices = jnp.concatenate([jnp.arange(batch) + batch, jnp.arange(batch)])\n",
        "  positives = logits[jnp.arange(2 * batch), pos_indices]\n",
        "  mask = ~jnp.eye(2 * batch, dtype=bool)\n",
        "  logits_masked = jnp.where(mask, logits, -jnp.inf)\n",
        "  logsumexp = jax.nn.logsumexp(logits_masked, axis=1)\n",
        "  loss = -positives + logsumexp\n",
        "  return jnp.mean(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Wm2m6xZCvS0"
      },
      "source": [
        "Verify your implementation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "PiKEMFQyARZT"
      },
      "outputs": [],
      "source": [
        "your_output = NTXent(Z1, Z2, 0.07)\n",
        "desired_output = 6.792835\n",
        "\n",
        "np.testing.assert_array_almost_equal(your_output, desired_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "_xG8i4BSH1Ri"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jDu2uOaEspK",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "### Hint 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaSmGkAsrqNb"
      },
      "source": [
        "Depending on how you programmed your cosine similarity, it may be more straight-forward to just recompute the similarities here instead of using your own function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEFWWqTfrunL",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "### Hint 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cz0cu-GNrzp1"
      },
      "source": [
        "The denominator contains all similarities between (i,k). Only the self-similarity (i,i) is left out. The similarity between (i,j) that's in the nominator is included in the denominator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0zXALj9rzNs",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "### Hint 3\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pR3_fYHsr0bF"
      },
      "source": [
        "You must compute the terms for both image pairs (i,j) _and_ (j,i). They will generally be different, even though on first glance they seem very similar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVjt4SNyrz8H",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "### Hint 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TJ8xwdxGUtI"
      },
      "source": [
        "It's quite useful to have a matrix of all pairwise cosine similarities to compute everything you need."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VF3QqWiQkvk8",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "### Hint 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4GGmk4LkzNi"
      },
      "source": [
        "Depending on how you implemented the cosine similarity, you might be able to compute the matrix of pairwise cosine similarities with the following line:\n",
        "`cosine_similarity(jnp.expand_dims(Zs, axis=1), jnp.expand_dims(Zs, axis=0))`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBoUg6L-DDOl"
      },
      "source": [
        "## Define the model\n",
        "\n",
        "After coding the loss function, we now need a model that we can train with it!\n",
        "\n",
        "We use a simple ResNet18 as our encoder model. It ends in a global average pooling, followed by a linear layer. We discard the linear layer and stick a projection head to the end of the model instead.\n",
        "\n",
        "You could also make use of that layer instead of removing it and then attaching a new linear layer as part of the projection head, but it's a bit cleaner this way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "q0J0kysj6cP5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "938bba17-c6d1-4d73-d4cc-a9623aeaa38c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((8, 2), dtype('float32'))"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# Load randomly initialized ResNet.\n",
        "\n",
        "# n_classes doesn't matter, we remove the only layer impacted by it anyway.\n",
        "# But we need to pass it to construct a ResNet.\n",
        "base_model = jax_resnet.ResNet18(n_classes=2)\n",
        "\n",
        "# Initialize the model's parameters. A dummy input is required to infer which\n",
        "# shape the parameters need to have\n",
        "random_key_1, *subkeys = random.split(random_key_0, 3)\n",
        "dummy_input = random.normal(subkeys[0],(8,32,32,3))\n",
        "base_params = base_model.init(rngs=subkeys[1], x=dummy_input)\n",
        "\n",
        "# Test whether everything works,\n",
        "# mutable=False ensures that batch_norm statistics aren't updated\n",
        "output = base_model.apply(base_params, dummy_input, mutable=False)\n",
        "output.shape, output.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "pm4n9NcYjzcg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da0cb97a-637a-45da-b97b-6e7a4adf079b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((8, 128), dtype('float32'))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# Source: https://www.kaggle.com/code/roguekk007/flax-model-surgery\n",
        "\n",
        "class EncoderWithProjectionHead(nn.Module):\n",
        "    encoder : nn.Sequential\n",
        "\n",
        "    def setup(self):\n",
        "        self.proj_head = nn.Sequential([\n",
        "            nn.Dense(features=128),\n",
        "            nn.relu,\n",
        "            nn.Dense(features=128)\n",
        "            ])\n",
        "\n",
        "    def __call__(self, x):\n",
        "        x = self.encoder(x)\n",
        "        return self.proj_head(x)\n",
        "\n",
        "# Use all of ResNet, except for the last layer\n",
        "encoder = nn.Sequential(base_model.layers[:-1])\n",
        "encoder_with_proj_head = EncoderWithProjectionHead(encoder=encoder)\n",
        "\n",
        "random_key_2, subkey = jax.random.split(random_key_1)\n",
        "pretrain_params = encoder_with_proj_head.init(subkey, dummy_input)\n",
        "\n",
        "# Overwrite the randomly initialized parameters with the ones we got from pretraining\n",
        "pretrain_params = unfreeze(pretrain_params)\n",
        "pretrain_params['params']['backbone'] = base_params[\"params\"]\n",
        "pretrain_params['batch_stats']['backbone'] = base_params[\"batch_stats\"]\n",
        "pretrain_params = freeze(pretrain_params)\n",
        "\n",
        "# Test whether everything works\n",
        "output = encoder_with_proj_head.apply(pretrain_params, dummy_input, mutable=False)\n",
        "output.shape, output.dtype"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPJiOE1joICF"
      },
      "source": [
        "## Train the model using your loss function\n",
        "\n",
        "Let's see what your loss function can do in practice! This should not take more than 2 minutes per epoch. If it does, make sure you are using a GPU environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "dnS_hX7l2Kfu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cb97492-80d6-4c6d-b869-925e3e5bd101"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 391/391 [03:06<00:00,  2.10it/s, train_loss=0.352]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: pretrain loss 3.1193060874938965\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 391/391 [02:25<00:00,  2.69it/s, train_loss=0.152]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: pretrain loss 0.30158573389053345\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 391/391 [02:25<00:00,  2.68it/s, train_loss=0.158]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: pretrain loss 0.1608607918024063\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 391/391 [02:26<00:00,  2.68it/s, train_loss=0.0315]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: pretrain loss 0.12118303775787354\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 391/391 [02:26<00:00,  2.66it/s, train_loss=0.0334]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: pretrain loss 0.09572690725326538\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "config = {\"learning_rate\": 0.01, \"momentum\": 0.9}\n",
        "pretrain_dataloader = NumpyLoader(pretrain_dataset, batch_size=128)\n",
        "\n",
        "pretrain_state = create_train_state(encoder_with_proj_head, pretrain_params, config)\n",
        "\n",
        "for epoch in range(5):\n",
        "\n",
        "  pretrain_state, pretrain_loss = pretrain_epoch(pretrain_state,\n",
        "                                                 pretrain_dataloader)\n",
        "  print(f\"Epoch {epoch}: pretrain loss {pretrain_loss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyXN8S4GgZcY"
      },
      "source": [
        "Hopefully you saw the loss going down! This is contrastive learning in practice. Simply by comparing positive and negative samples, the model gets trained. Of course, we still need to evaluate how useful the learned features are.\n",
        "\n",
        " But before that: Why do we even bother with negative pairs?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMlom8ZV1bGi"
      },
      "source": [
        "## Intermezzo: Focusing on the positive\n",
        "\n",
        "\n",
        "**Task 5**\n",
        "1. What would we need to change in our loss function to only use positive pairs?\n",
        "2. What do you think the effect would be on training?\n",
        "\n",
        "Please answer this question before running the code below, which will give you the answer. You don't need to guess right, but you are expected to think about it and make an educated guess based on your understanding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGrJd_drhyvO"
      },
      "source": [
        "**Answer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FFYMjWp-PFR"
      },
      "source": [
        "**Task 6** Quickly write the loss function that only uses positive pairs, then run the code below to see whether your prediction was right."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "MrZiHNgf9gk8"
      },
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def PositivesOnlyLoss(Z1, Z2):\n",
        "  Z1 = Z1 / jnp.maximum(jnp.linalg.norm(Z1, axis=1, keepdims=True), 1e-12)\n",
        "  Z2 = Z2 / jnp.maximum(jnp.linalg.norm(Z2, axis=1, keepdims=True), 1e-12)\n",
        "  cos_sim = jnp.sum(Z1 * Z2, axis=1)\n",
        "  return -jnp.mean(cos_sim)\n",
        "\n",
        "# Verify your solution\n",
        "your_output = PositivesOnlyLoss(Z1, Z2)\n",
        "desired_output = -0.06671171\n",
        "\n",
        "np.testing.assert_array_almost_equal(your_output, desired_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "dQxYmS1i9Xg9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a266a4d-ec8d-4041-8e02-8b9cc6dac706"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 391/391 [02:59<00:00,  2.18it/s, train_loss=-0.997]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: -0.9849804639816284\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Train model using only positive pairs\n",
        "\n",
        "# Note that we can use the same model and parameters as before, since these were\n",
        "# not changed by the previous training. The updated parameters are stuck in\n",
        "# pretrain_state for now. This is part of the functional character of JAX.\n",
        "positives_only_state = create_train_state(encoder_with_proj_head,\n",
        "                                          pretrain_params, config)\n",
        "\n",
        "for epoch in range(1):\n",
        "\n",
        "  positives_only_state, pretrain_loss = pretrain_epoch(positives_only_state,\n",
        "                                                       pretrain_dataloader,\n",
        "                                                       positives_only=True)\n",
        "  print(f\"Epoch {epoch}: {jnp.mean(pretrain_loss)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2i1oN_gQhsCo"
      },
      "source": [
        "**Task 7** What did you observe? What does the loss value tell you about the average relationship of the compared embeddings? How useful would you expect the learned embeddings to be on unseen data?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNAzVkBqhwGu"
      },
      "source": [
        "**Answer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGeyeeUD7RPc",
        "tags": []
      },
      "source": [
        "There are actually some more recent methods that *do* manage to use only positive pairs, without encoutering the problem you saw happen above. If you're interested how, look at one of these: [BYOL](http://arxiv.org/abs/2006.07733), [Barlow Twins](https://arxiv.org/abs/2103.03230), [SimSiam](https://arxiv.org/abs/2011.10566)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyDYu7gWdPfI"
      },
      "source": [
        "## Linear evaluation - How good are the features we learned?\n",
        "\n",
        "To judge how good the features are that we learned, we want to test how useful they are for performing the task we actually care about. For this, we use the **linear evaluation** protocol. We take the trained model, discard the projection head and freeze all the model weights. We then add a single linear layer to perform our prediction task and evaluate its performance.\n",
        "\n",
        "In JAX, freezing the weights is currently done by setting the gradients to zero. We have implemented that as part of create_train_state, so you will not see it in the code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "O2LP40Mu52LQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bef407a4-e876-4f74-ed72-ca4e7e631dbd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((8, 10), dtype('float32'))"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "# Source: https://www.kaggle.com/code/roguekk007/flax-model-surgery\n",
        "\n",
        "class AddClassificationLayerToBackbone(nn.Module):\n",
        "    backbone : nn.Sequential\n",
        "    num_classes : int\n",
        "\n",
        "    def setup(self):\n",
        "        self.head = nn.Dense(self.num_classes)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        x = self.backbone(x)\n",
        "        return self.head(x)\n",
        "\n",
        "backbone = encoder_with_proj_head.encoder\n",
        "finetune_model = AddClassificationLayerToBackbone(backbone=backbone,\n",
        "                                                  num_classes=10)\n",
        "\n",
        "random_key_3, subkey = jax.random.split(random_key_2)\n",
        "finetune_params = finetune_model.init(subkey, dummy_input)\n",
        "\n",
        "# Note: This unfreeze has nothing to do with the notion of freezing parameters.\n",
        "# It only allows us to write into an otherwise write-protected FrozenDict.\n",
        "finetune_params = unfreeze(finetune_params)\n",
        "\n",
        "# Overwrite the randomly initialized parameters with the ones we got from pretraining\n",
        "finetune_params['params']['backbone'] = pretrain_state.params[\"encoder\"]\n",
        "finetune_params['batch_stats']['backbone'] = pretrain_state.batch_stats[\"encoder\"]\n",
        "finetune_params = freeze(finetune_params)\n",
        "\n",
        "# Test whether everything works\n",
        "output = finetune_model.apply(finetune_params, dummy_input, mutable=False)\n",
        "output.shape, output.dtype"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXN3KSdj-oZN"
      },
      "source": [
        "### Finetune the model's last layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "fS57etKU_XGq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "606afbeb-8e89-47d0-fbb3-f5ed25ecb4b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:09<00:00,  2.48s/it, train_loss=2.5, train_acc=0.0862069]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: train loss 2.54, train accuracy 0.10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:03<00:00, 19.94it/s, val_loss=2.49, val_acc=0.0625]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: val loss: 2.45, val accuracy: 0.10\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00,  7.87it/s, train_loss=2.34, train_acc=0.18103448]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: train loss 2.34, train accuracy 0.15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:01<00:00, 71.68it/s, val_loss=2.5, val_acc=0.0625]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: val loss: 2.32, val accuracy: 0.14\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00,  5.71it/s, train_loss=2.16, train_acc=0.2672414]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: train loss 2.18, train accuracy 0.22\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:01<00:00, 53.09it/s, val_loss=2.38, val_acc=0.0625]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: val loss: 2.21, val accuracy: 0.19\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00,  7.51it/s, train_loss=2, train_acc=0.31034482]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: train loss 2.01, train accuracy 0.30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:01<00:00, 63.92it/s, val_loss=2.23, val_acc=0.188]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: val loss: 2.14, val accuracy: 0.23\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00,  7.41it/s, train_loss=1.92, train_acc=0.33620688]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: train loss 1.94, train accuracy 0.33\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:01<00:00, 66.93it/s, val_loss=2.2, val_acc=0.188]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: val loss: 2.14, val accuracy: 0.24\n",
            "\n"
          ]
        }
      ],
      "source": [
        "supervised_train_dataloader = NumpyLoader(supervised_dataset_1p, batch_size=128)\n",
        "val_dataloader = NumpyLoader(val_dataset, batch_size=128)\n",
        "\n",
        "finetune_config = {\"learning_rate\": 0.01, \"momentum\": 0.9}\n",
        "finetune_state = create_train_state(finetune_model, finetune_params, finetune_config, freeze_encoder=True)\n",
        "\n",
        "for epoch in range(5):\n",
        "  finetune_state, train_loss, train_acc = supervised_epoch(state=finetune_state, train_dl=supervised_train_dataloader)\n",
        "  print(f\"Epoch {epoch}: train loss {train_loss:.2f}, train accuracy {train_acc:.2f}\")\n",
        "\n",
        "  val_loss, val_acc = compute_validation_performance(finetune_state, val_dataloader)\n",
        "  print(f\"Epoch {epoch}: val loss: {val_loss:.2f}, val accuracy: {val_acc:.2f}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmGce4T4BWZJ"
      },
      "source": [
        "## Comparison to other methods\n",
        "\n",
        "How well did our self-supervised training work? Below, you can see some values for comparison. All of them have only been run for a comparatively short time. The value in brackets indicates how many labeled images have been used in supervised training or supervised finetuning.\n",
        "- Linear evaluation from random initialization and from an ImageNet-pretrained model only train the final linear layer of the network for 5 epochs.\n",
        "- Fully supervised training starts from a randomly initialized network and trains the whole network for 5 epochs.\n",
        "- SimCLR trains for 20 epochs on the full unlabeled training dataset. Then, the final linear layer is trained for 5 epochs.\n",
        "\n",
        "| Method                                 \t| Test set accuracy \t|\n",
        "|----------------------------------------\t|-------------------\t|\n",
        "| Linear evaluation from random initialization (100% labels)         \t|     0.22              \t|\n",
        "| Linear evaluation from ImageNet pretraining (1% labels) |                    0.34                 \t|\n",
        "| Linear evaluation from ImageNet pretraining (100% labels) |                   0.41                 \t|\n",
        "| Full supervised training (1% labels)   \t|   0.33                \t|\n",
        "| Full supervised training (100% labels) \t|   0.57                \t|\n",
        "| SimCLR training (1% labels)   \t|    0.25               \t|\n",
        "| SimCLR training (100% labels)   \t|  0.33                 \t|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfP8qK4tqf2P"
      },
      "source": [
        "## Self-supervised learning: Great if you have few labels\n",
        "\n",
        "Using self-supervision, we can make use of unlabeled data that would be useless to supervised approaches. This way, we can often achieve much better performance than pure supervised learning would achieve, especially if the labeled dataset is comparatively small. Depending on the setting, self-supervised approaches can even perform better than pure supervised approaches without using any additional unlabeled data.\n",
        "\n",
        "**Task 8** In this exercise, the self-supervised model did not work as well as promised in the paper. Why is that? If you had access to any ressources you need, how would you change the setup to increase the performance and hopefully beat supervised training?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuQ-t5o4z_MW"
      },
      "source": [
        "**Answer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IWWfgplJimF"
      },
      "source": [
        "**Task 9** How long did it take you to complete this practical? This information is valuable to us to balance the difficulty of different practicals."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjrFKhgGJlhm"
      },
      "source": [
        "**Answer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHSIxSRCeGzh"
      },
      "source": [
        "# References\n",
        "[1] [Chen, Ting, et al. \"A simple framework for contrastive learning of visual representations.\" International conference on machine learning. PMLR, 2020.](https://proceedings.mlr.press/v119/chen20j.html)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "_jDu2uOaEspK",
        "YEFWWqTfrunL",
        "B0zXALj9rzNs",
        "lVjt4SNyrz8H",
        "VF3QqWiQkvk8"
      ],
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}